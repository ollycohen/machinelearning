{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw3_code.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kLbL-LlnOAgm"},"source":["# Homework 3 Code"]},{"cell_type":"code","metadata":{"id":"0aSWrpaElDM0","executionInfo":{"status":"ok","timestamp":1605283857608,"user_tz":360,"elapsed":1322,"user":{"displayName":"Oliver Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNu_beV2PK6EJXIuxzc4IcrztaZt-cGiECTEaVhQ=s64","userId":"04909173470536449687"}}},"source":["# Add import statements here\n","import numpy as np, matplotlib as plt, pandas as pd, time\n","from scipy import stats"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"TDvZ5xhsl9XM","executionInfo":{"status":"ok","timestamp":1605283899401,"user_tz":360,"elapsed":41780,"user":{"displayName":"Oliver Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNu_beV2PK6EJXIuxzc4IcrztaZt-cGiECTEaVhQ=s64","userId":"04909173470536449687"}},"outputId":"ba75e7c8-1948-4da5-dfe8-d2872f271f8a","colab":{"base_uri":"https://localhost:8080/"}},"source":["# To access files in your Google Drive, run this block and follow the instructions\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Hi_dRrJDmtiI","executionInfo":{"status":"ok","timestamp":1605288181329,"user_tz":360,"elapsed":1457,"user":{"displayName":"Oliver Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNu_beV2PK6EJXIuxzc4IcrztaZt-cGiECTEaVhQ=s64","userId":"04909173470536449687"}},"outputId":"102cf449-1f20-4ff6-a439-3f1fa2e16210","colab":{"base_uri":"https://localhost:8080/"}},"source":["# To test if the above block worked, run this block\n","!ls '/content/gdrive/My Drive/Wash U Fall 2020/417T Intro to Machine Learning F2020/CSV_Files'"],"execution_count":4,"outputs":[{"output_type":"stream","text":["cleveland_test.csv  cleveland_train.csv\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i1rrY8fIxz7h"},"source":[" ## Find test error\n","\n","The `find_test_error` function computes the test error of a linear classifier $w$. \n","\n","The hypothesis is assumed to be of the form $sign([1, x(N,:)] \\cdot w)$.\n","\n","Inputs:\n","* `w` is the weight vector\n","* `X` is the data matrix (without an initial column of 1's)\n","* `y` are the data labels (plus or minus 1)\n","\n","Outputs:\n","* `test_error` is the binary error of $w$ on the data set $(X, y)$ error; this should be between 0 and 1. "]},{"cell_type":"code","metadata":{"id":"0BCKbvjMlHtE","executionInfo":{"status":"ok","timestamp":1605288249530,"user_tz":360,"elapsed":307,"user":{"displayName":"Oliver Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNu_beV2PK6EJXIuxzc4IcrztaZt-cGiECTEaVhQ=s64","userId":"04909173470536449687"}}},"source":["def find_test_error(w, X, y):\n","\n","  N = X.shape[0]\n","  col = X.shape[1]\n","  ones = np.ones(N)\n","  X = np.c_[ones, X]\n","  \n","  #Calcuate the binary error 1/N * Sum ( sign(w*x) != y | 1; else 0)\n","  hx = np.sign(np.matmul(X,w))\n","  test_error = (hx - y)/2          # 1-1=0, 1-(-1)=2\n","  test_error = abs(np.sum(test_error) / test_error.shape[0])\n","\n","  #Equivalent sigmoid function\n","  # xw = np.matmul(X, w)\n","  # yxw = y*xw\n","  # sigmoid = np.exp(yxw)/(1+np.exp(yxw))\n","  # test_error = np.sum(sigmoid > 0.5)/N  #The cutoff is 0.5\n","\n","  return test_error"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JUF6Mr1V0S5T"},"source":[" ## Logistic Regression\n","\n","The `logistic_reg`  learn a logistic regression model using gradient descent.\n","\n","Inputs:\n","* `X` is the data matrix (without an initial column of 1's)\n","* `y` are the data labels (plus or minus 1)\n","* `w_init` is the initial value of the w vector ($d+1$ dimensional)\n","* `max_its` is the maximum number of iterations to run for\n","* `eta` is the learning rate\n","\n","Outputs:\n","* t is the number of iterations gradient descent ran for\n","* w is the learned weight vector\n","* e_in is the in-sample (cross-entropy) error "]},{"cell_type":"code","metadata":{"id":"dTcJkPE6lHvg","executionInfo":{"status":"ok","timestamp":1605288976686,"user_tz":360,"elapsed":345,"user":{"displayName":"Oliver Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNu_beV2PK6EJXIuxzc4IcrztaZt-cGiECTEaVhQ=s64","userId":"04909173470536449687"}}},"source":["def logistic_reg(X, y, w_init, max_its, eta):\n","  \n","  #Add ones to X\n","  N = X.shape[0]\n","  ones = np.ones(N)\n","  X = np.c_[ones, X]\n","  col = X.shape[1]\n","\n","  #Set up variables to start\n","  eps = 10**(-6)\n","  t = 0\n","  w = w_init\n","  gt = np.ones(col) #Gradient vector with number of elements equal to weights\n","\n","  num = y*X # numerator of the gradient\n","  #Run the gradient descent algorithm to find the weights\n","  while np.nanmax(abs(gt)) > eps and t < max_it:  #while less than max iterations and magnitude of one element of gradient > eps\n","    h = np.matmul(X, w)\n","    gt = 1+np.exp(h*y) # denominator of gradient, constant value, changes after each iteration\n","    gt = (1/N)*np.sum(num/gt, axis=0)\n","    gt = np.reshape(gt, (col,1))\n","    w += gt*eta #update weights\n","    t+=1 #update iterations\n","\n","\n","  r1 = X[0]\n","  r1 = w*r1[:,np.newaxis]\n","  h = np.matmul(X, w)\n","  e_in = np.log(1+np.exp(-y*h))\n","  #print(X, r1, np.matmul(X[0],w))\n","  e_in = 1/N*np.sum(e_in)\n","\n","  #clean it up\n","  w = w[:,:]\n","  return t, w, e_in, np.nanmax(abs(gt))"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q7d-boqb0y_H"},"source":["## Run and Plot\n","\n","Run your code and plot figures below"]},{"cell_type":"code","metadata":{"id":"FWHPRXv4lHx6","executionInfo":{"status":"ok","timestamp":1605289029146,"user_tz":360,"elapsed":46267,"user":{"displayName":"Oliver Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNu_beV2PK6EJXIuxzc4IcrztaZt-cGiECTEaVhQ=s64","userId":"04909173470536449687"}},"outputId":"831d09b2-9001-41b0-81f2-c138f0a908a9","colab":{"base_uri":"https://localhost:8080/"}},"source":["X = np.genfromtxt('/content/gdrive/My Drive/Wash U Fall 2020/417T Intro to Machine Learning F2020/CSV_Files/cleveland_train.csv', skip_header=True, delimiter=',')\n","X_test = np.genfromtxt('/content/gdrive/My Drive/Wash U Fall 2020/417T Intro to Machine Learning F2020/CSV_Files/cleveland_test.csv', skip_header=True, delimiter=',')\n","\n","col = X.shape[1] \n","y = X[:,col-1]          #Initialize y vector\n","y = np.where(y==0,-1,y) #In y vector, change all 0s to -1s\n","y = y.reshape(y.shape[0],1)\n","X = X[:,0:col-1]          #Remove y vector from X\n","\n","#Test set\n","y_test = X_test[:,col-1]\n","y_test = np.where(y_test==0,-1,y_test)\n","y_test = y_test.reshape(y_test.shape[0],1)\n","X_test = X_test[:,0:col-1]\n","\n","w = np.zeros((col,1)) #initialize weight vector to zeros\n","\n","X_z = stats.zscore(X, axis=0)\n","X_testZ = stats.zmap(X_test, X, axis=0)\n","\n","eta = 10**(-5)\n","max_it = 10**6\n","start_time = time.time()\n","t, w, e_in, stp_cdn = logistic_reg(X_z ,y,w,max_it, eta)\n","end_time = time.time()\n","time_passed = abs(start_time - end_time)\n","test_error_training_set = find_test_error(w, X_z, y)\n","test_error_test_set = find_test_error(w, X_testZ, y_test)\n","print(\"Learning rate {}: \\n E_in is {}. \\n The binary error on training set is {}. \\n The binary error on test set is {}. \\n The training process took {} seconds over {} iterations. \\n Stop condition was at {}\"\n",".format(eta, e_in, test_error_training_set, test_error_test_set, time_passed, t, stp_cdn))"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Learning rate 1e-05: \n"," E_in is 0.4166924959018434. \n"," The binary error on training set is 0.006578947368421052. \n"," The binary error on test set is 0.06896551724137931. \n"," The training process took 45.64273929595947 seconds over 1000000 iterations. \n"," Stop condition was at 0.02221937354957484\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"meDA4n45796P","executionInfo":{"status":"ok","timestamp":1605247465041,"user_tz":360,"elapsed":345,"user":{"displayName":"Oliver Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNu_beV2PK6EJXIuxzc4IcrztaZt-cGiECTEaVhQ=s64","userId":"04909173470536449687"}},"outputId":"296b93a0-3263-4e2e-95df-c1e032cb562d","colab":{"base_uri":"https://localhost:8080/"}},"source":["w"],"execution_count":237,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.00052699],\n","       [ 0.00682633],\n","       [ 0.04066563],\n","       [ 0.10819079],\n","       [ 0.01244496],\n","       [-0.00082465],\n","       [-0.01090669],\n","       [ 0.0394019 ],\n","       [-0.02658727],\n","       [ 0.0377911 ],\n","       [ 0.10391188],\n","       [ 0.03779678],\n","       [ 0.11746569],\n","       [ 0.25456501]])"]},"metadata":{"tags":[]},"execution_count":237}]}]}